# nv30_infra
nv30 Infra repository

## Homework-11: [![Build Status](https://travis-ci.com/Otus-DevOps-2018-09/nv30_infra.svg?branch=ansible-4)](https://travis-ci.com/Otus-DevOps-2018-09/nv30_infra)

 - На хост с Ubuntu 18.10 установлены Ansible, Vagrant, Molecule, Testinfra и VirtualBox, используя файл requirements.txt.
 - Создан файл конфигурации Vagrant, где определены виртуальные машины app и db.
 - В конфигурацию Vagrant добавлен провижининг для app и db, для чего понадобилось доработать ansible роли app и db, а также создать плейбук base.yml, для установки python. В доработку ролей входил перенос тасков из main.yml в отдельные файлы и добавления тегов к отдельным задачам внутри тасков, для возможности более гибкого использования получившихся ролей.
 - Попутно проведена параметризация роли app, где появилась переменная deploy_user для указания пользователя для деплоя приложения.
 - \*Для работы проксирования с помощью nginx в Vagrantfile в разделе **ansible.extra_vars** определена переменная **nginx_sites** с необходимыми параметрами:
 <pre><code>  ansible.extra_vars = {
    ...
    &quot;nginx_sites&quot; =&gt; {
      &quot;default&quot; =&gt; [
        &quot;listen 80&quot;,
        &quot;server_name 'reddit'&quot;,
        &quot;location / { proxy_pass http://127.0.0.1:9292; }&quot;</code></pre>
 - Создана заготовка для тестов роли db. Добавлены два тесты для проверки статуса службы mongod и наличия файла конфигурации MongoDB по пути "/etc/mongod.conf", в котором должен присутствовать параметр "bindIp: 0.0.0.0".
 - Создана тестовая машина. На нее применен playbook.yml в котором вызывается роль db. Произведен прогон тестов (успешно).
   - _Проблемы_: Molecule не могла запустить созданную машину, т.к. не могла получить доступ к директории "/usr". В ней она пыталась создать log файл о запуске машины, чтобы потом на него линковать serial и таким образом получать энтропию для "/dev/random". Первый вариант решения проблемы с отключением serial в конфигурации molecule.yml не помог. Машина запускалась, но загрузиться до конца так и не могла. Для быстрого решения проблемы создал log файл в папке "/usr" сам и дал на него права пользователю, от которого Molecula запускает Vagrant и создает виртуальную машину.
 - Добавлен тест для проверки, что MongoDB слушает на порту 27017 по tcp со всех источников:
  <pre><code>  def test_mongo_listening(host):
    mongo_tcp = host.socket("tcp://0.0.0.0:27017")
    assert mongo_tcp.is_listening</code></pre>
 - В плейбуках packer_app.yml и packer_db.yml таски полностью заменены на ссылки на роли app и db. В шаблонах packer указаны теги, для запуска только нужных во время сборки образа задач.
    - _Проблемы_: Билд packer падал, т.к. не видел ролей в папке ansible. Помогло указание пути к папке с ролями, используя соответствующие переменные:
     <pre><code>  "ansible_env_vars": ["ANSIBLE_ROLES_PATH=ansible/roles"]</code></pre>
 - \*Роль db вынесена в [отдельный репозиторий](https://github.com/nv30/db) и её подключение описано в файле requirements.yml обоих окружений.
 - \*Для созданного репозитория [настроен и подключен Travis CI]( https://travis-ci.com/nv30/db/builds) и добавлен badge о статусе сборки.
 - \*Новый репозиторий db и Travis CI для него, подключены к существующему чату Slack для отправки оповещений о коммитах и сборках.

## Homework-10: [![Build Status](https://travis-ci.com/Otus-DevOps-2018-09/nv30_infra.svg?branch=ansible-3)](https://travis-ci.com/Otus-DevOps-2018-09/nv30_infra)

 - Созданы заготовки для ролей app и db с помощью команды "ansible-galaxy init".
 - Описаны роли app и db, используя task'и и handler'ы из плейбуков app.yml и db.yml.
 - В плейбуках app.yml и db.yml заменены task'и и handler'ы на вызов ролей app и db.
 - Созданы окружения stage и prod. В ansible.cfg окружением по умолчанию указан stage.
 - Созданы переменные групп хостов для окружений, используя переменные из плейбуков app.yml и db.yml.
 - Для удобства добавлен вывод информации о том, в каком окружении находится конфигурируемый хост.
 - Организована папка ansible. Файлы из прошлых ДЗ перемещены в папку old.
 - Улучшен файл ansible.cfg с помощью нескольких полезных параметров.
 - В окружениях созданы файлы requirements.yml с указанием на установку коммьюнити роли jdauphant.nginx версии 2.21.1. Роль добавлена в .gitignore.
 - Для работы роли добавлены минимально необходимые переменные в файлы групп хостов app в stage и prod окружениях.
 - В конфигурации окружений в terraform обновлено правило файерволла для открытия порта http.
 - В плейбук app.yml добавлен вызов роли jdauphant.nginx.
 - Для работы с ansible vault создан файл vault.key с ключом для шифрования. В ansible.cfg добавлена опция vault_password_file со ссылкой на этот файл.
 - Создан плейбук users.yml для добавления пользователей. В окружениях созданы файлы credentials.yml с именами и паролями пользователей. Файлы зашифрованы с помощью ключа vault.key командой "ansible-vault encrypt".
 - \*Настроено использование dynamic inventory для окружений stage и prod с помощью плагина **gcp_compute**. Подробное описание этого плагина можно прочитать в блоке о ДЗ#9.
 - \**Travis CI настроен для контроля состояния инфраструктурного репозитория nv30_infra. В .travis.yml описана установка необходимых утилит и зависимостей, скрипты для проверки шаблонов packer, конфигураций terraform для окружений stage и prod и плейбуков ansible. Добавлен badge со статусом билда в Travis CI. Для отладки использовалась утилита trytravis с тестовым репозиторием nv30_infra_trytravis. 

## Homework-9:

 - Отключен провижининг приложения из HW-7 путем изменения переменной deploy_app в false.
 - Создан плейбук reddit_app.yml с одним сценарием для управления конфигурацией MongoDB, используя шаблон файла mongod.conf.
 - Определена переменная mongo_bind_ip, которая используется в шаблоне mongod.conf.j2 для указания на каких интерфейсах "слушать".
 - Создан handler для рестарта службы mongod после изменения файла конфигурации.
 - В сценарий добавлен task для копирования unit файла puma.service на хост приложения.
 - Создан handler для рестарта службы puma после изменения unit файла.
 - Создан шаблон db_config.j2, где описано присвоение переменной DATABASE_URL значения из Ansible переменной db_host.
 - В сценарий добавлен task для копирования созданного шаблона на хост приложения.
 - В сценарий добавлены task'и для деплоя приложения, используя модули git и bundle.
 - Создан плейбук reddit_app2.yml с несколькими сценариями для конфигурирования хостов приложения и БД.
 - Создан сценарий для деплоя приложения.
 - Созданы отдельные плейбуки для конфигурирования хостов приложения и БД, а также деплоя приложения.
 - Создан плейбук site.yml в котором описано управление конфигурацией всей инфраструктуры, используя плейбуки, созданные выше.
 - \*Для dynamic inventory решено использовать плагин **gcp_compute**, который разработан совместно Ansible и Google. Для его использования необходимы библиотеки requests и google-auth, которые можно установить через pip. В ansible.cfg **gce_compute** добавлен в перечне плагинов для инвентори и стоит на первом месте в порядке применения. Для применения плагина создан инвентори файл inventory.gcp.yml. После применения получаем хосты разбитые на группы по тегу, который к ним применяется. Название группы состоит из префикса tag и имени тега, например tag_reddit_app. В нашем случае dynamic inventory используется для получения групп хостов app и db, на которые потом применяются плейбуки app, db и deploy. А также для получения внутреннего ip хоста db, который далее используется для определения переменной db_host и далее DATABASE_URL в шаблоне db_config.j2.
 - Изменен провижининг в packer шаблонах app и db на использование ansible плейбуков packer_app.yml и packer_db.yml.
 - Используя обновленные конфигурации packer собраны новые образы на gce.
 - Используя terraform развернута инфраструктура на обновленных образах. К хостам применен плейбук site.yml. Проверена работа приложения.


## Homework-8:

 - Установлен python, pip, ansible и необходимые зависимости.
 - Используя конфиги из ДЗ terraform-2, развернуто stage окружение в GCP.
 - Создан инвентори файл inventory, где указана информация об инстансе reddit-app-stage и параметры подключения к нему по ssh. 
 - В inventory добавлена информация об инстансе reddit-db-stage. 
 - С помощью модуля ping проверено подключение к обоим инстансам.
 - Создан файл конфигурации ansible.cfg, где указаны параметры подключения к инстансам по ssh. Теперь их можно удалить из файла inventory.
 - Файл inventory изменен, хосты разбиты по группам app и db.
 - Создан файл inventory.yml, где с помощью YAML описана конфигурация из файла inventory.
 - Протестировано несколько модулей и команд, для понимания преимуществ использования модулей.
 - Создан плейбук clone.yml, где описано клонирование репозитория с приложением reddit, используя модуль git.
   - _Нюансы_: При повторном выполнении плейбука он таки склонировал репо, т.к. мы удалили папку reddit, в которую клонировавли репо до этого. Поэтому статус выполнения плейбука из "ok=2 changed=0" изменился в "ok=2 changed=1".
 - \*Создан inventory.json, где в необходимом для ansible формате описано содержимое inventory.yml. Также сделан простой bash скрипт show_json.sh, который с помощью утилиты cat отдает содержимое файла inventory.json. Команда "ansible all -m ping -i show_json.sh" выполняется успешно.

## Homework-7:

 - В конфигурации инстанса определен ресурс файерволла, для создания правила доступа к нему по ssh.
 - В state файл импортирована информация о существующем аналогичном правиле файерволла, чтобы не возникало ошибки alreadyExists.
 - Определен ресурс google_compute_address для создания статического внешнего IP адреса в GCP. В конфигурации ресурса инстанса добавлена ссылка на ресурс google_compute_address и его атрибут address. Теперь инстанс использует созданный статический IP как свой внешний адрес.
 - Созданы шаблоны packer для образов app (установлен Ruby) и db (установлена MongoDB). С помощью шаблонов созданы образы reddit-app-base и reddit-db-base в GCP.
 - Созданы отдельные конфигурации Terraform app.tf и db.tf, для разворачивания инстансов из образов reddit-app-base и reddit-db-base.
 - Создана конфигурация vpc.tf, где определен ресурс файерволла и описано создание правила для доступа к инстансам по ssh.
 - Созданы модули app, db и vpc со своими переменными и выходными параметрами. Переопределена выходная переменная для внешнего IP инстанса, и теперь она ссылается на атрибут модуля app.
 - Произведена параметризация модулей с помощью input переменных.
 - Созданы отдельные директории для stage и prod окружений.
   - _Проблемы_: Нельзя было развернуть оба окружения одновременно, т.к. имена правил для файерволла были идентичные. Поэтому были параметризированы имена для правил, чтобы к ним добавлялось имя окружения, в котором они создаются. Таким же образом были параметризированы: имена инстансов app и db, имя внешнего IP для app, имя внутреннего IP для db.
 - Создана конфигурация storage-bucket.tf, где с помощью модуля storage-bucket из реестра модулей Terraform описано создание бакета в сервисе GCS.
 - \*Настроено хранение state файла в удаленном бекенде, используя созданный ранее бакет в сервисе GSC. Описание бекенда хранится в файле backend.tf.
   - _Проблемы_: При одновременном запуске двух идентичных конфигураций Terraform из разных мест, получил ошибку из-за невозможности правильно "залочить стейт" - **Error 412: Precondition Failed, conditionNotMet**.
 - \*В модули app и db добавлены провиженеры для деплоя приложения.
   - _Проблемы_: Переменную окружения DATABASE_URL определил немного читерским способом, добавив ее в секцию [Service] файла службы puma. Разработчики рекомендуют использовать это для удобного дебага, но почему бы и нет? Сам IP параметризовал и получаю его из модуля db. Также, столкнулся с тем, что по умолчанию служба mongodb слушает входящие только на интерфейсе 127.0.0.1. Было решено с помощью **sed** заменить 127.0.0.1 на 0.0.0.0, что и добавил в конфигурацию модуля db в провиженер remote-exec.
 - \*Отключение провиженеров реализовал с помощью **null_resource**. Провиженеры переместил в ресурс null_resource, где есть ключ count, указывающий на переменную deploy_app. Значение true этой переменной интерпретируется Terraform'ом как 1, а false как 0. Чтобы провиженеры запускались каждый раз когда deploy_app=true, сделал триггер build_number указывающий на ${timestamp(), который изменяется при каждом запуске Terraform.

## Homework-6:

 - Удалены ssh ключи из метаданных проекта.
 - Установлен Terraform v0.11.10. Загружен провайдер "google" v.1.4.0.
 - В main.tf описано создание одной VM из образа reddit-base. VM развернута.
 - В main.tf добавлено описание добавления ssh ключа в разворачиваемую **VM** (instance).
 - Добавлен outputs.tf, где описываются нужные нам выходные переменные из terraform.tfstate.
 - В main.tf добавлено описание создания правила tcp:9292 для FW. Добавлен тег для создаваемой VM, чтобы применялось это правило.
 - Созданы файлы deploy.sh и puma.service для деплоя приложения и создания systemd unit для веб сервера Puma.
 - В main.tf добавлены провиженеры для копирования файлов deploy.sh и puma.service на VM.
 - Проверена работа провиженеров, приложение деплоится и работает. На него можно зайти по http://app_external_ip:9292.
 - Добавлен variables.tf, в котором описаны нужные нам входные переменные. Соответствующие параметры в main.tf заменены ссылками на эти переменные.
 - Добавлен terraform.tfvars, где определяются переменные.
 - Добавлены переменные для приватного ключа и зоны.
 - Конфигурационные файлы отформатированы командой "terraform fmt".
 - Создан файл terraform.tfvars.example с примерами описаний переменных.
 - \*Описано добавление нескольких ключей пользователей в **метаданные проекта**.
   - _Проблемы_: При добавлении ключа пользователя в метаданные проекта через веб, Terraform при применении конфига через "terraform apply" затирает его теми, что описаны в конфигурационном файле.
 - \**Создан lb.tf с описанием создания HTTP балансировщика. Балансировщик развернут и приложение доступно на его ip.
 - \**Описано создание нескольких VM без лишнего кода, используя параметр **count**.
   - _Проблемы_: БД на разных VM в группе не синхронизированы, что делает балансировку нагрузки бессмысленной. Также, без применения параметра count, конфигурационный файл разрастается из-за лишнего описания каждой VM.

## Homework-5:

 - Установлен packer.
 - Создан ADC.
 - Создан шаблон для packer для образа семейства reddit-base.
 - Созданы скрипты для провижининга (установка Ruby и MongoDB).
 - Параметризирован шаблон для reddit-base. Переменные определены в шаблоне и заданы в отдельном файле variables.json.
 - Добавлены дополнительные опции builder для GCP (описание образа, размер и тип диска, сеть, теги, пользователь ssh).
 - Создан образ семейства reddit-full, в который запечены все необходимые для работы приложения зависимости.
 - Создана vm на базе образа reddit-full и проверена работа приложения "из коробки".
 - Создан скрипт для создания vm на базе образа reddit-full.

## Homework-4:

Команда для создания инстанса с использованием startup-script:

```
gcloud compute instances create reddit-app \
        --boot-disk-size=10GB \
        --image-family ubuntu-1604-lts \
        --image-project=ubuntu-os-cloud \
        --machine-type=g1-small \
        --tags puma-server \
        --restart-on-failure \
        --metadata-from-file startup-script=startup.sh
```

Команда для создания правила для файерволла:

```
gcloud compute firewall-rules create default-puma-server \
    --action allow \
    --direction ingress \
    --rules tcp:9292 \
    --source-ranges 0.0.0.0/0 \
    --target-tags puma-server
```

```
testapp_IP=35.204.222.243
testapp_port=9292
```

## Homework-3:

Для подключения к someinternalhost используем jumphost, применяя ProxyCommand. Jumphost'ом выступает хост bastion.
Для этого необходимо создать файл ~/.ssh/config со следующим содержимым:

```
Host bastion
  User nv30
  Hostname 35.204.199.208

Host someinternalhost
  User nv30
  Hostname 10.164.0.3
  Port 22
  ProxyCommand ssh -q -W %h:%p bastion
```

Таким образом сразу же выполняется дополнительное задание, т.к. в config заданы алиасы для хостов bastion и someinternalhost.
Для подключения к bastion через openvpn:

```
bastion_IP = 35.204.199.208
someinternalhost_IP = 10.164.0.3
```

